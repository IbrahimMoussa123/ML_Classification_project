{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d30a0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>pu_location</th>\n",
       "      <th>do_location</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>...</th>\n",
       "      <th>vendor_VeriFone Inc.</th>\n",
       "      <th>store_and_fwd_flag_N</th>\n",
       "      <th>store_and_fwd_flag_Y</th>\n",
       "      <th>rate_type_Standard rate</th>\n",
       "      <th>rate_type_Negotiated fare</th>\n",
       "      <th>payment_type_Credit card</th>\n",
       "      <th>payment_type_Cash</th>\n",
       "      <th>trip_type_Street-hail</th>\n",
       "      <th>trip_type_Dispatch</th>\n",
       "      <th>price_per_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:18:50</td>\n",
       "      <td>2018-01-01 00:24:39</td>\n",
       "      <td>134</td>\n",
       "      <td>159</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:30:26</td>\n",
       "      <td>2018-01-01 00:46:42</td>\n",
       "      <td>107</td>\n",
       "      <td>109</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:07:25</td>\n",
       "      <td>2018-01-01 00:19:45</td>\n",
       "      <td>109</td>\n",
       "      <td>138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.280374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:32:40</td>\n",
       "      <td>2018-01-01 00:33:41</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>210.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 00:38:35</td>\n",
       "      <td>2018-01-01 01:08:50</td>\n",
       "      <td>100</td>\n",
       "      <td>141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.63</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.960924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  lpep_pickup_datetime lpep_dropoff_datetime  pu_location  do_location  \\\n",
       "0  2018-01-01 00:18:50   2018-01-01 00:24:39          134          159   \n",
       "1  2018-01-01 00:30:26   2018-01-01 00:46:42          107          109   \n",
       "2  2018-01-01 00:07:25   2018-01-01 00:19:45          109          138   \n",
       "3  2018-01-01 00:32:40   2018-01-01 00:33:41          100          100   \n",
       "4  2018-01-01 00:38:35   2018-01-01 01:08:50          100          141   \n",
       "\n",
       "   passenger_count  trip_distance  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0              5.0           0.70          6.0    0.5      0.5         0.0   \n",
       "1              5.0           3.50         14.5    0.5      0.5         0.0   \n",
       "2              1.0           2.14         10.0    0.5      0.5         0.0   \n",
       "3              1.0           0.03          4.5   -0.5     -0.5         0.0   \n",
       "4              1.0           5.63         21.0    0.5      0.5         0.0   \n",
       "\n",
       "   ...  vendor_VeriFone Inc.  store_and_fwd_flag_N  store_and_fwd_flag_Y  \\\n",
       "0  ...                     1                     1                     0   \n",
       "1  ...                     1                     1                     0   \n",
       "2  ...                     1                     1                     0   \n",
       "3  ...                     1                     1                     0   \n",
       "4  ...                     1                     1                     0   \n",
       "\n",
       "   rate_type_Standard rate  rate_type_Negotiated fare  \\\n",
       "0                        1                          0   \n",
       "1                        1                          0   \n",
       "2                        1                          0   \n",
       "3                        1                          0   \n",
       "4                        1                          0   \n",
       "\n",
       "   payment_type_Credit card  payment_type_Cash  trip_type_Street-hail  \\\n",
       "0                         0                  1                      1   \n",
       "1                         0                  1                      1   \n",
       "2                         0                  1                      1   \n",
       "3                         0                  0                      1   \n",
       "4                         0                  1                      1   \n",
       "\n",
       "   trip_type_Dispatch  price_per_m  \n",
       "0                   0    10.428571  \n",
       "1                   0     4.514286  \n",
       "2                   0     5.280374  \n",
       "3                   0   210.000000  \n",
       "4                   0     3.960924  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\Lenovo\\Desktop\\Ibrahim\\dataEngProject\\milestone1\\clean_df2.csv')\n",
    "\n",
    "df['trip_distance'] = df['trip_distance'].replace(0, 0.01)\n",
    "\n",
    "df['price_per_m'] = df['total_amount'] / df['trip_distance']\n",
    "\n",
    "quantiles = df['price_per_m'].quantile([0.33, 0.66])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095bafa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pu_location</th>\n",
       "      <th>do_location</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>hours</th>\n",
       "      <th>vendor_Creative Mobile Technologies, LLC</th>\n",
       "      <th>vendor_VeriFone Inc.</th>\n",
       "      <th>store_and_fwd_flag_N</th>\n",
       "      <th>store_and_fwd_flag_Y</th>\n",
       "      <th>rate_type_Standard rate</th>\n",
       "      <th>rate_type_Negotiated fare</th>\n",
       "      <th>payment_type_Credit card</th>\n",
       "      <th>payment_type_Cash</th>\n",
       "      <th>trip_type_Street-hail</th>\n",
       "      <th>trip_type_Dispatch</th>\n",
       "      <th>price_per_m</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134</td>\n",
       "      <td>159</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.428571</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>109</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.514286</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.280374</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.960924</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pu_location  do_location  passenger_count  trip_distance  hours  \\\n",
       "0          134          159              5.0           0.70      0   \n",
       "1          107          109              5.0           3.50      0   \n",
       "2          109          138              1.0           2.14      0   \n",
       "3          100          100              1.0           0.03      0   \n",
       "4          100          141              1.0           5.63      0   \n",
       "\n",
       "   vendor_Creative Mobile Technologies, LLC  vendor_VeriFone Inc.  \\\n",
       "0                                         0                     1   \n",
       "1                                         0                     1   \n",
       "2                                         0                     1   \n",
       "3                                         0                     1   \n",
       "4                                         0                     1   \n",
       "\n",
       "   store_and_fwd_flag_N  store_and_fwd_flag_Y  rate_type_Standard rate  \\\n",
       "0                     1                     0                        1   \n",
       "1                     1                     0                        1   \n",
       "2                     1                     0                        1   \n",
       "3                     1                     0                        1   \n",
       "4                     1                     0                        1   \n",
       "\n",
       "   rate_type_Negotiated fare  payment_type_Credit card  payment_type_Cash  \\\n",
       "0                          0                         0                  1   \n",
       "1                          0                         0                  1   \n",
       "2                          0                         0                  1   \n",
       "3                          0                         0                  0   \n",
       "4                          0                         0                  1   \n",
       "\n",
       "   trip_type_Street-hail  trip_type_Dispatch  price_per_m  day_of_week  month  \n",
       "0                      1                   0    10.428571            0      1  \n",
       "1                      1                   0     4.514286            0      1  \n",
       "2                      1                   0     5.280374            0      1  \n",
       "3                      1                   0   210.000000            0      1  \n",
       "4                      1                   0     3.960924            0      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lpep_pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "df['lpep_dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n",
    "df['day_of_week'] = df['lpep_pickup_datetime'].dt.dayofweek\n",
    "df['month'] = df['lpep_pickup_datetime'].dt.month\n",
    "\n",
    "df.drop(columns=['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadf2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_price_quantiles(row):\n",
    "    if row['price_per_m'] <= quantiles[0.33]:\n",
    "        return 'low'\n",
    "    elif row['price_per_m'] <= quantiles[0.66]:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b98a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pu_location</th>\n",
       "      <th>do_location</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>hours</th>\n",
       "      <th>vendor_Creative Mobile Technologies, LLC</th>\n",
       "      <th>vendor_VeriFone Inc.</th>\n",
       "      <th>store_and_fwd_flag_N</th>\n",
       "      <th>store_and_fwd_flag_Y</th>\n",
       "      <th>rate_type_Standard rate</th>\n",
       "      <th>rate_type_Negotiated fare</th>\n",
       "      <th>payment_type_Credit card</th>\n",
       "      <th>payment_type_Cash</th>\n",
       "      <th>trip_type_Street-hail</th>\n",
       "      <th>trip_type_Dispatch</th>\n",
       "      <th>price_per_m</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>price_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134</td>\n",
       "      <td>159</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.428571</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>109</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.514286</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.280374</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.960924</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pu_location  do_location  passenger_count  trip_distance  hours  \\\n",
       "0          134          159              5.0           0.70      0   \n",
       "1          107          109              5.0           3.50      0   \n",
       "2          109          138              1.0           2.14      0   \n",
       "3          100          100              1.0           0.03      0   \n",
       "4          100          141              1.0           5.63      0   \n",
       "\n",
       "   vendor_Creative Mobile Technologies, LLC  vendor_VeriFone Inc.  \\\n",
       "0                                         0                     1   \n",
       "1                                         0                     1   \n",
       "2                                         0                     1   \n",
       "3                                         0                     1   \n",
       "4                                         0                     1   \n",
       "\n",
       "   store_and_fwd_flag_N  store_and_fwd_flag_Y  rate_type_Standard rate  \\\n",
       "0                     1                     0                        1   \n",
       "1                     1                     0                        1   \n",
       "2                     1                     0                        1   \n",
       "3                     1                     0                        1   \n",
       "4                     1                     0                        1   \n",
       "\n",
       "   rate_type_Negotiated fare  payment_type_Credit card  payment_type_Cash  \\\n",
       "0                          0                         0                  1   \n",
       "1                          0                         0                  1   \n",
       "2                          0                         0                  1   \n",
       "3                          0                         0                  0   \n",
       "4                          0                         0                  1   \n",
       "\n",
       "   trip_type_Street-hail  trip_type_Dispatch  price_per_m  day_of_week  month  \\\n",
       "0                      1                   0    10.428571            0      1   \n",
       "1                      1                   0     4.514286            0      1   \n",
       "2                      1                   0     5.280374            0      1   \n",
       "3                      1                   0   210.000000            0      1   \n",
       "4                      1                   0     3.960924            0      1   \n",
       "\n",
       "  price_category  \n",
       "0           high  \n",
       "1            low  \n",
       "2            low  \n",
       "3           high  \n",
       "4            low  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price_category'] = df.apply(categorize_price_quantiles, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "226a795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['pu_location', 'do_location', 'passenger_count', 'trip_distance', 'store_and_fwd_flag_N', 'store_and_fwd_flag_Y', 'rate_type_Standard rate', 'rate_type_Negotiated fare', 'hours', 'vendor_Creative Mobile Technologies, LLC', 'vendor_VeriFone Inc.', 'payment_type_Credit card', 'payment_type_Cash', 'trip_type_Street-hail', 'trip_type_Dispatch', 'day_of_week', 'month']]\n",
    "label = df['price_category']\n",
    "label = label.astype('category').cat.codes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.3, random_state=42)\n",
    "#y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d1f21",
   "metadata": {},
   "source": [
    "# Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdfc2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "589977f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  gini\n",
      "Accuracy: 0.8220129656637526\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87     80841\n",
      "           1       0.88      0.84      0.86     78164\n",
      "           2       0.71      0.78      0.75     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82333824 0.82194093 0.821903   0.8228514  0.82178287]\n",
      "Mean accuracy: 0.822363288041932\n",
      "Standard deviation: 0.0006190201868315179\n",
      "Accuracy test on train: 0.8257361854025649\n",
      " \n",
      "parameters:  entropy\n",
      "Accuracy: 0.8205629694573473\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87     80841\n",
      "           1       0.88      0.84      0.86     78164\n",
      "           2       0.71      0.79      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82180816 0.82061317 0.82082814 0.82228868 0.8204804 ]\n",
      "Mean accuracy: 0.8212037101434614\n",
      "Standard deviation: 0.000714910446802193\n",
      "Accuracy test on train: 0.8233317616965007\n",
      " \n",
      "parameters:  log_loss\n",
      "Accuracy: 0.8205629694573473\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87     80841\n",
      "           1       0.88      0.84      0.86     78164\n",
      "           2       0.71      0.79      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82179551 0.82061317 0.82082814 0.82230133 0.8204804 ]\n",
      "Mean accuracy: 0.8212037101434614\n",
      "Standard deviation: 0.0007166529564757444\n",
      "Accuracy test on train: 0.8233317616965007\n",
      " \n",
      "highest accuracy:  0.8220129656637526\n",
      "cross validation score 0.822363288041932\n",
      "best parameters accuracy:  gini\n",
      "Accuracy: 0.8257361854025649\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87    188017\n",
      "           1       0.88      0.84      0.86    182815\n",
      "           2       0.71      0.79      0.75    182731\n",
      "\n",
      "    accuracy                           0.83    553563\n",
      "   macro avg       0.83      0.83      0.83    553563\n",
      "weighted avg       0.83      0.83      0.83    553563\n",
      "\n",
      "lowest accuracy:  0.8205629694573473\n",
      "cross validation score 0.8212037101434614\n",
      "worst parameters:  entropy\n",
      "Accuracy: 0.8233317616965007\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87    188017\n",
      "           1       0.88      0.84      0.86    182815\n",
      "           2       0.71      0.79      0.75    182731\n",
      "\n",
      "    accuracy                           0.82    553563\n",
      "   macro avg       0.83      0.82      0.83    553563\n",
      "weighted avg       0.83      0.82      0.83    553563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Criterions = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "accuracy = []\n",
    "params = []\n",
    "cross_val = []\n",
    "\n",
    "for criterion in Criterions:\n",
    "\n",
    "    parameters = (criterion)\n",
    "    print(\"parameters: \", parameters)\n",
    "    params.append(parameters)\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion=criterion, max_depth= 10, min_samples_split= 10)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, features, label, cv=kf, scoring='accuracy')\n",
    "    cross_val.append(scores.mean())\n",
    "    print(f\"Cross-validation scores: {scores}\")\n",
    "    print(f\"Mean accuracy: {scores.mean()}\")\n",
    "    print(f\"Standard deviation: {scores.std()}\")\n",
    "\n",
    "    y_pred = clf.predict(X_train)\n",
    "    print(\"Accuracy test on train:\", accuracy_score(y_train, y_pred))\n",
    "\n",
    "    print(\" \")\n",
    "\n",
    "print(\"highest accuracy: \", max(accuracy))\n",
    "\n",
    "max_index = accuracy.index(max(accuracy))\n",
    "best_param = params[max_index]\n",
    "print(\"cross validation score\", cross_val[max_index])\n",
    "print(\"best parameters accuracy: \", best_param)\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=best_param, max_depth= 10, min_samples_split= 10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_pred))\n",
    "\n",
    "print(\"lowest accuracy: \", min(accuracy))\n",
    "\n",
    "min_index = accuracy.index(min(accuracy))\n",
    "worst_param = params[min_index]\n",
    "print(\"cross validation score\", cross_val[min_index])\n",
    "print(\"worst parameters: \", worst_param)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=worst_param, max_depth= 10, min_samples_split= 10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d777b",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2f09c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "features = df[['pu_location', 'do_location', 'passenger_count', 'trip_distance', 'store_and_fwd_flag_N', 'store_and_fwd_flag_Y', 'rate_type_Standard rate', 'rate_type_Negotiated fare', 'hours', 'vendor_Creative Mobile Technologies, LLC', 'vendor_VeriFone Inc.', 'payment_type_Credit card', 'payment_type_Cash', 'trip_type_Street-hail', 'trip_type_Dispatch', 'day_of_week', 'month']]\n",
    "\n",
    "label = df['price_category']\n",
    "label = label.astype('category').cat.codes\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e214f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  gini\n",
      "Accuracy: 0.8220003203479991\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87     80841\n",
      "           1       0.88      0.84      0.86     78164\n",
      "           2       0.71      0.78      0.75     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82333824 0.82194093 0.82191564 0.8228514  0.82178287]\n",
      "Mean accuracy: 0.8223658171104127\n",
      "Standard deviation: 0.0006171574873950873\n",
      "Accuracy test on train: 0.8257361854025649\n",
      " \n",
      "parameters:  entropy\n",
      "Accuracy: 0.8205587543520961\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87     80841\n",
      "           1       0.88      0.84      0.86     78164\n",
      "           2       0.71      0.79      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82180183 0.82061317 0.82082814 0.82228868 0.82048672]\n",
      "Mean accuracy: 0.8212037101434614\n",
      "Standard deviation: 0.0007125692521014628\n",
      "Accuracy test on train: 0.8233317616965007\n",
      " \n",
      "parameters:  log_loss\n",
      "Accuracy: 0.8205756147731009\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87     80841\n",
      "           1       0.88      0.84      0.86     78164\n",
      "           2       0.71      0.79      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82180816 0.82061317 0.82082814 0.82230133 0.82048672]\n",
      "Mean accuracy: 0.8212075037461828\n",
      "Standard deviation: 0.000717482509109622\n",
      "Accuracy test on train: 0.8233281487382647\n",
      " \n",
      "highest accuracy:  0.8220003203479991\n",
      "cross validation score 0.8223658171104127\n",
      "best parameters accuracy:  gini\n",
      "Accuracy: 0.8257361854025649\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87    188017\n",
      "           1       0.88      0.84      0.86    182815\n",
      "           2       0.71      0.79      0.75    182731\n",
      "\n",
      "    accuracy                           0.83    553563\n",
      "   macro avg       0.83      0.83      0.83    553563\n",
      "weighted avg       0.83      0.83      0.83    553563\n",
      "\n",
      "lowest accuracy:  0.8205587543520961\n",
      "cross validation score 0.8212037101434614\n",
      "worst parameters:  entropy\n",
      "Accuracy: 0.8233317616965007\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87    188017\n",
      "           1       0.88      0.84      0.86    182815\n",
      "           2       0.71      0.79      0.75    182731\n",
      "\n",
      "    accuracy                           0.82    553563\n",
      "   macro avg       0.83      0.82      0.83    553563\n",
      "weighted avg       0.83      0.82      0.83    553563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Criterions = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "accuracy = []\n",
    "params = []\n",
    "cross_val = []\n",
    "\n",
    "for criterion in Criterions:\n",
    "\n",
    "    parameters = (criterion)\n",
    "    print(\"parameters: \", parameters)\n",
    "    params.append(parameters)\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion=criterion, max_depth= 10, min_samples_split= 10)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, features, label, cv=kf, scoring='accuracy')\n",
    "    cross_val.append(scores.mean())\n",
    "    print(f\"Cross-validation scores: {scores}\")\n",
    "    print(f\"Mean accuracy: {scores.mean()}\")\n",
    "    print(f\"Standard deviation: {scores.std()}\")\n",
    "\n",
    "    y_pred = clf.predict(X_train)\n",
    "    print(\"Accuracy test on train:\", accuracy_score(y_train, y_pred))\n",
    "\n",
    "    print(\" \")\n",
    "\n",
    "print(\"highest accuracy: \", max(accuracy))\n",
    "\n",
    "max_index = accuracy.index(max(accuracy))\n",
    "best_param = params[max_index]\n",
    "print(\"cross validation score\", cross_val[max_index])\n",
    "print(\"best parameters accuracy: \", best_param)\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=best_param, max_depth= 10, min_samples_split= 10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_pred))\n",
    "\n",
    "print(\"lowest accuracy: \", min(accuracy))\n",
    "\n",
    "min_index = accuracy.index(min(accuracy))\n",
    "worst_param = params[min_index]\n",
    "print(\"cross validation score\", cross_val[min_index])\n",
    "print(\"worst parameters: \", worst_param)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=worst_param, max_depth= 10, min_samples_split= 10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b8889",
   "metadata": {},
   "source": [
    "# Random Forest Classifiaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca01b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1346981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  (20, 'gini')\n",
      "Accuracy: 0.8205798298783521\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.71      0.77      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82058156 0.81934864 0.81901354 0.82107473 0.81815365]\n",
      "Mean accuracy: 0.8196344231510929\n",
      "Standard deviation: 0.001061262950989165\n",
      "Accuracy test on train: 0.8220401291271273\n",
      " \n",
      "parameters:  (20, 'entropy')\n",
      "Accuracy: 0.8186155908313031\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.72      0.75      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81831804 0.81581427 0.81850772 0.81857095 0.81664254]\n",
      "Mean accuracy: 0.8175707032707178\n",
      "Standard deviation: 0.001129919191066121\n",
      "Accuracy test on train: 0.8209706934892687\n",
      " \n",
      "parameters:  (20, 'log_loss')\n",
      "Accuracy: 0.8115215686935703\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86     80841\n",
      "           1       0.84      0.88      0.86     78164\n",
      "           2       0.73      0.70      0.71     78237\n",
      "\n",
      "    accuracy                           0.81    237242\n",
      "   macro avg       0.81      0.81      0.81    237242\n",
      "weighted avg       0.81      0.81      0.81    237242\n",
      "\n",
      "Cross-validation scores: [0.81437902 0.81509348 0.81398701 0.82042982 0.81662357]\n",
      "Mean accuracy: 0.8161025790175834\n",
      "Standard deviation: 0.0023435425815735347\n",
      "Accuracy test on train: 0.8136273558745798\n",
      " \n",
      "parameters:  (40, 'gini')\n",
      "Accuracy: 0.819467042092041\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.77      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82149835 0.81993032 0.81946245 0.82214326 0.81921586]\n",
      "Mean accuracy: 0.8204500477361677\n",
      "Standard deviation: 0.0011605669023560235\n",
      "Accuracy test on train: 0.8214114743940617\n",
      " \n",
      "parameters:  (40, 'entropy')\n",
      "Accuracy: 0.8179327437806122\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81691441 0.81881121 0.81530845 0.81883018 0.81904515]\n",
      "Mean accuracy: 0.8177818804888689\n",
      "Standard deviation: 0.0014577145733582306\n",
      "Accuracy test on train: 0.8201108094290984\n",
      " \n",
      "parameters:  (40, 'log_loss')\n",
      "Accuracy: 0.8178442265703375\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.72      0.75      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81471412 0.81632008 0.81745816 0.81855198 0.81763519]\n",
      "Mean accuracy: 0.816935907082024\n",
      "Standard deviation: 0.0013182585991562388\n",
      "Accuracy test on train: 0.8198163533328636\n",
      " \n",
      "parameters:  (60, 'gini')\n",
      "Accuracy: 0.8211404388767588\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.78      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82171964 0.82039188 0.82003149 0.82114428 0.81941819]\n",
      "Mean accuracy: 0.8205410942014784\n",
      "Standard deviation: 0.0008118723445692545\n",
      "Accuracy test on train: 0.822672396818429\n",
      " \n",
      "parameters:  (60, 'entropy')\n",
      "Accuracy: 0.8177472791495604\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81819791 0.81842553 0.81884282 0.81745184 0.81917793]\n",
      "Mean accuracy: 0.8184192057460435\n",
      "Standard deviation: 0.0005899021736309456\n",
      "Accuracy test on train: 0.8200584215346762\n",
      " \n",
      "parameters:  (60, 'log_loss')\n",
      "Accuracy: 0.8172583269404237\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.73      0.72      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81568149 0.8159344  0.81870373 0.8212897  0.81686383]\n",
      "Mean accuracy: 0.8176946276262795\n",
      "Standard deviation: 0.002087303665165326\n",
      "Accuracy test on train: 0.8197603524802055\n",
      " \n",
      "parameters:  (80, 'gini')\n",
      "Accuracy: 0.8203564293000396\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.77      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82099253 0.81947509 0.82173861 0.82089137 0.82119486]\n",
      "Mean accuracy: 0.8208584922958252\n",
      "Standard deviation: 0.0007510779867899162\n",
      "Accuracy test on train: 0.8221683891445056\n",
      " \n",
      "parameters:  (80, 'entropy')\n",
      "Accuracy: 0.8167735898365382\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.88      0.86     78164\n",
      "           2       0.72      0.72      0.72     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81680692 0.81953832 0.81865314 0.81963948 0.81719893]\n",
      "Mean accuracy: 0.818367359842186\n",
      "Standard deviation: 0.0011722880345279268\n",
      "Accuracy test on train: 0.8193918307401326\n",
      " \n",
      "parameters:  (80, 'log_loss')\n",
      "Accuracy: 0.8196693671440975\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.72      0.75      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82030336 0.81715467 0.81650976 0.81882386 0.81886179]\n",
      "Mean accuracy: 0.8183306883492136\n",
      "Standard deviation: 0.0013502241625597804\n",
      "Accuracy test on train: 0.8213825707281737\n",
      " \n",
      "parameters:  (100, 'gini')\n",
      "Accuracy: 0.821098287824247\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.78      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82144144 0.82037291 0.82165009 0.82216223 0.8205942 ]\n",
      "Mean accuracy: 0.821244175239155\n",
      "Standard deviation: 0.0006675423436321796\n",
      "Accuracy test on train: 0.8229939501014338\n",
      " \n",
      "parameters:  (100, 'entropy')\n",
      "Accuracy: 0.8182573068849529\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.84      0.88      0.86     78164\n",
      "           2       0.73      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81857727 0.8191716  0.81951935 0.82037291 0.81803352]\n",
      "Mean accuracy: 0.8191349321261245\n",
      "Standard deviation: 0.0008007248086843308\n",
      "Accuracy test on train: 0.8202950702991348\n",
      " \n",
      "parameters:  (100, 'log_loss')\n",
      "Accuracy: 0.8171740248354001\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86     80841\n",
      "           1       0.85      0.88      0.86     78164\n",
      "           2       0.73      0.72      0.72     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81823585 0.81984181 0.8189187  0.81920954 0.81853301]\n",
      "Mean accuracy: 0.8189477810585416\n",
      "Standard deviation: 0.0005563318354825553\n",
      "Accuracy test on train: 0.8198217727702176\n",
      " \n",
      "parameters:  (120, 'gini')\n",
      "Accuracy: 0.820980264877214\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.77      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82125809 0.8200631  0.82104944 0.82200416 0.82015162]\n",
      "Mean accuracy: 0.8209052800627209\n",
      "Standard deviation: 0.0007252674375146099\n",
      "Accuracy test on train: 0.8227807855655093\n",
      " \n",
      "parameters:  (120, 'entropy')\n",
      "Accuracy: 0.8190244560406673\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.74      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81776797 0.81799559 0.81865314 0.8211506  0.81918425]\n",
      "Mean accuracy: 0.8189503101270225\n",
      "Standard deviation: 0.001207785773641505\n",
      "Accuracy test on train: 0.821098953506647\n",
      " \n",
      "parameters:  (120, 'log_loss')\n",
      "Accuracy: 0.8175786749395132\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81866579 0.81769842 0.81896928 0.81948141 0.81809675]\n",
      "Mean accuracy: 0.8185823306630585\n",
      "Standard deviation: 0.0006296237762213826\n",
      "Accuracy test on train: 0.820354684110029\n",
      " \n",
      "parameters:  (140, 'gini')\n",
      "Accuracy: 0.8209844799824652\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.78      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82076492 0.81977226 0.8208724  0.8210115  0.81989239]\n",
      "Mean accuracy: 0.8204626930785718\n",
      "Standard deviation: 0.0005219846627465695\n",
      "Accuracy test on train: 0.8225965246954727\n",
      " \n",
      "parameters:  (140, 'entropy')\n",
      "Accuracy: 0.8183036730427159\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81886812 0.81900089 0.81922218 0.81892502 0.81661092]\n",
      "Mean accuracy: 0.8185254266222394\n",
      "Standard deviation: 0.0009647774739883495\n",
      "Accuracy test on train: 0.8206473337271458\n",
      " \n",
      "parameters:  (140, 'log_loss')\n",
      "Accuracy: 0.8184554168317583\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86     80841\n",
      "           1       0.86      0.87      0.86     78164\n",
      "           2       0.72      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81790075 0.81888708 0.81900089 0.81910838 0.81893134]\n",
      "Mean accuracy: 0.8187656881279203\n",
      "Standard deviation: 0.0004388753680635088\n",
      "Accuracy test on train: 0.8209490157398526\n",
      " \n",
      "parameters:  (160, 'gini')\n",
      "Accuracy: 0.8209254685089487\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.78      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82242146 0.82061317 0.82163745 0.82046775 0.82034762]\n",
      "Mean accuracy: 0.8210974892672656\n",
      "Standard deviation: 0.0008047208181633853\n",
      "Accuracy test on train: 0.8225892987790008\n",
      " \n",
      "parameters:  (160, 'entropy')\n",
      "Accuracy: 0.819420675934278\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.74      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81881121 0.81785649 0.81930438 0.81884282 0.81859624]\n",
      "Mean accuracy: 0.8186822288680522\n",
      "Standard deviation: 0.0004730203260566828\n",
      "Accuracy test on train: 0.8217944479670787\n",
      " \n",
      "parameters:  (160, 'log_loss')\n",
      "Accuracy: 0.8188347763043643\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.74      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82019588 0.81886812 0.81797662 0.81843817 0.81822953]\n",
      "Mean accuracy: 0.8187416619773522\n",
      "Standard deviation: 0.0007836338175230665\n",
      "Accuracy test on train: 0.8212452783152053\n",
      " \n",
      "parameters:  (180, 'gini')\n",
      "Accuracy: 0.8200908776692154\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.77      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82134028 0.81942451 0.82079653 0.82132763 0.82037923]\n",
      "Mean accuracy: 0.8206536377488762\n",
      "Standard deviation: 0.0007116485934978059\n",
      "Accuracy test on train: 0.8217709637385446\n",
      " \n",
      "parameters:  (180, 'entropy')\n",
      "Accuracy: 0.817287832677182\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81949406 0.81902618 0.81973432 0.81893134 0.81882386]\n",
      "Mean accuracy: 0.8192019524408671\n",
      "Standard deviation: 0.00035094869169922835\n",
      "Accuracy test on train: 0.8197585460010876\n",
      " \n",
      "parameters:  (180, 'log_loss')\n",
      "Accuracy: 0.8183121032532182\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.74      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81786913 0.81875431 0.81940554 0.81912734 0.81867844]\n",
      "Mean accuracy: 0.8187669526621606\n",
      "Standard deviation: 0.0005199527383518425\n",
      "Accuracy test on train: 0.8205208801888855\n",
      " \n",
      "highest accuracy:  0.8211404388767588\n",
      "cross validation score 0.8205410942014784\n",
      "best parameters accuracy:  (60, 'gini')\n"
     ]
    }
   ],
   "source": [
    "estimators = np.arange(20, 200, 20)\n",
    "Criterions = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "accuracy = []\n",
    "params = []\n",
    "cross_val = []\n",
    "\n",
    "for n_est in estimators:\n",
    "    for criterion in Criterions:\n",
    "\n",
    "        parameters = (n_est, criterion)\n",
    "        print(\"parameters: \", parameters)\n",
    "        params.append(parameters)\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=n_est, criterion=criterion,max_depth= 10, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = rf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(rf, features, label, cv=kf, scoring='accuracy')\n",
    "        cross_val.append(scores.mean())\n",
    "        print(f\"Cross-validation scores: {scores}\")\n",
    "        print(f\"Mean accuracy: {scores.mean()}\")\n",
    "        print(f\"Standard deviation: {scores.std()}\")\n",
    "\n",
    "        y_pred = rf.predict(X_train)\n",
    "        print(\"Accuracy test on train:\", accuracy_score(y_train, y_pred))\n",
    "\n",
    "        print(\" \")\n",
    "            \n",
    "print(\"highest accuracy: \", max(accuracy))\n",
    "\n",
    "max_index = accuracy.index(max(accuracy))\n",
    "best_param = params[max_index]\n",
    "print(\"cross validation score\", cross_val[max_index])\n",
    "print(\"best parameters accuracy: \", best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4332abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest accuracy:  0.8115215686935703\n",
      "cross validation score 0.8161025790175834\n",
      "worst parameters:  (20, 'log_loss')\n"
     ]
    }
   ],
   "source": [
    "print(\"lowest accuracy: \", min(accuracy))\n",
    "\n",
    "min_index = accuracy.index(min(accuracy))\n",
    "worst_param = params[min_index]\n",
    "print(\"cross validation score\", cross_val[min_index])\n",
    "print(\"worst parameters: \", worst_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9cb9aa",
   "metadata": {},
   "source": [
    "### Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbb04c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "features = df[['pu_location', 'do_location', 'passenger_count', 'trip_distance', 'store_and_fwd_flag_N', 'store_and_fwd_flag_Y', 'rate_type_Standard rate', 'rate_type_Negotiated fare', 'hours', 'vendor_Creative Mobile Technologies, LLC', 'vendor_VeriFone Inc.', 'payment_type_Credit card', 'payment_type_Cash', 'trip_type_Street-hail', 'trip_type_Dispatch', 'day_of_week', 'month']]\n",
    "\n",
    "label = df['price_category']\n",
    "label = label.astype('category').cat.codes\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2334ada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  (20, 'gini')\n",
      "Accuracy: 0.8196820124598512\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.78      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81920954 0.81881753 0.82012633 0.8202591  0.81778062]\n",
      "Mean accuracy: 0.8192386239338395\n",
      "Standard deviation: 0.0009092133093224404\n",
      "Accuracy test on train: 0.8213988290402358\n",
      " \n",
      "parameters:  (20, 'entropy')\n",
      "Accuracy: 0.8142698173173384\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.72      0.72     78237\n",
      "\n",
      "    accuracy                           0.81    237242\n",
      "   macro avg       0.81      0.81      0.81    237242\n",
      "weighted avg       0.81      0.81      0.81    237242\n",
      "\n",
      "Cross-validation scores: [0.81803352 0.81731906 0.81883018 0.81552342 0.81620627]\n",
      "Mean accuracy: 0.817182491258907\n",
      "Standard deviation: 0.001196937602708712\n",
      "Accuracy test on train: 0.8165412066919212\n",
      " \n",
      "parameters:  (20, 'log_loss')\n",
      "Accuracy: 0.8155259186821895\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.84      0.88      0.86     78164\n",
      "           2       0.72      0.72      0.72     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.81    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81723054 0.8174961  0.81885547 0.81733803 0.81404392]\n",
      "Mean accuracy: 0.8169928111228433\n",
      "Standard deviation: 0.0015871018072980804\n",
      "Accuracy test on train: 0.8172619918600051\n",
      " \n",
      "parameters:  (40, 'gini')\n",
      "Accuracy: 0.8215788098228812\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.78      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81924748 0.8191147  0.81977858 0.82117589 0.81889973]\n",
      "Mean accuracy: 0.8196432748907758\n",
      "Standard deviation: 0.0008192835272518216\n",
      "Accuracy test on train: 0.8228819483961175\n",
      " \n",
      "parameters:  (40, 'entropy')\n",
      "Accuracy: 0.8146533918951956\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.73      0.71      0.72     78237\n",
      "\n",
      "    accuracy                           0.81    237242\n",
      "   macro avg       0.81      0.81      0.81    237242\n",
      "weighted avg       0.81      0.81      0.81    237242\n",
      "\n",
      "Cross-validation scores: [0.81775533 0.81806514 0.81717364 0.81919689 0.81866579]\n",
      "Mean accuracy: 0.81817135703492\n",
      "Standard deviation: 0.0007037039315023503\n",
      "Accuracy test on train: 0.8167290805201938\n",
      " \n",
      "parameters:  (40, 'log_loss')\n",
      "Accuracy: 0.8173721347822055\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.88      0.86     78164\n",
      "           2       0.73      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.8168006  0.8150429  0.81848876 0.81973432 0.81697131]\n",
      "Mean accuracy: 0.8174075783537029\n",
      "Standard deviation: 0.0015958151569690504\n",
      "Accuracy test on train: 0.8195815110475231\n",
      " \n",
      "parameters:  (60, 'gini')\n",
      "Accuracy: 0.8206472715623709\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.78      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82210532 0.81942451 0.82002516 0.8213466  0.81947509]\n",
      "Mean accuracy: 0.8204753384209763\n",
      "Standard deviation: 0.0010698916252883735\n",
      "Accuracy test on train: 0.8226633644228389\n",
      " \n",
      "parameters:  (60, 'entropy')\n",
      "Accuracy: 0.8187546893045919\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.86     80841\n",
      "           1       0.85      0.88      0.86     78164\n",
      "           2       0.73      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81764152 0.81740126 0.81848243 0.81733171 0.81692073]\n",
      "Mean accuracy: 0.8175555288598326\n",
      "Standard deviation: 0.0005183449095083015\n",
      "Accuracy test on train: 0.8203835877759171\n",
      " \n",
      "parameters:  (60, 'log_loss')\n",
      "Accuracy: 0.8202721271950161\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.72      0.75      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81936761 0.81793236 0.81635169 0.82015162 0.81703454]\n",
      "Mean accuracy: 0.8181675634321989\n",
      "Standard deviation: 0.0014151341610329415\n",
      "Accuracy test on train: 0.8225802663834108\n",
      " \n",
      "parameters:  (80, 'gini')\n",
      "Accuracy: 0.8215745947176301\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.87     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.78      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82083447 0.82027175 0.82048672 0.82199784 0.82027175]\n",
      "Mean accuracy: 0.8207725039674761\n",
      "Standard deviation: 0.0006462920567549358\n",
      "Accuracy test on train: 0.8236497020212695\n",
      " \n",
      "parameters:  (80, 'entropy')\n",
      "Accuracy: 0.8151254836833276\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.73      0.72      0.72     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.81      0.81      0.81    237242\n",
      "weighted avg       0.81      0.82      0.81    237242\n",
      "\n",
      "Cross-validation scores: [0.81902618 0.81928541 0.81742655 0.81786281 0.81766049]\n",
      "Mean accuracy: 0.8182522872263073\n",
      "Standard deviation: 0.0007549873063333314\n",
      "Accuracy test on train: 0.8175040600618177\n",
      " \n",
      "parameters:  (80, 'log_loss')\n",
      "Accuracy: 0.8167735898365382\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.73      0.72      0.72     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81697764 0.81833069 0.81871637 0.81838127 0.81949406]\n",
      "Mean accuracy: 0.8183800051845903\n",
      "Standard deviation: 0.0008152706347386314\n",
      "Accuracy test on train: 0.8192238281821581\n",
      " \n",
      "parameters:  (100, 'gini')\n",
      "Accuracy: 0.8213048279815547\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.77      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82158054 0.82016426 0.82139718 0.82159951 0.82017059]\n",
      "Mean accuracy: 0.8209824166513868\n",
      "Standard deviation: 0.0006691835867783578\n",
      "Accuracy test on train: 0.8229505946026017\n",
      " \n",
      "parameters:  (100, 'entropy')\n",
      "Accuracy: 0.817169809730149\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81853934 0.81860256 0.81794501 0.81951935 0.81880489]\n",
      "Mean accuracy: 0.8186822288680522\n",
      "Standard deviation: 0.000507164942925877\n",
      "Accuracy test on train: 0.8195471879442809\n",
      " \n",
      "parameters:  (100, 'log_loss')\n",
      "Accuracy: 0.8177978604125745\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81918425 0.817749   0.81927909 0.81888076 0.81838759]\n",
      "Mean accuracy: 0.8186961387446969\n",
      "Standard deviation: 0.0005664689988124458\n",
      "Accuracy test on train: 0.8201252612620424\n",
      " \n",
      "parameters:  (120, 'gini')\n",
      "Accuracy: 0.8210729971927399\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.77      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82146041 0.81920954 0.82017691 0.821903   0.8202022 ]\n",
      "Mean accuracy: 0.8205904110368548\n",
      "Standard deviation: 0.0009703897673038375\n",
      "Accuracy test on train: 0.8225206525725166\n",
      " \n",
      "parameters:  (120, 'entropy')\n",
      "Accuracy: 0.8182530917797017\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.74      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81920954 0.81829907 0.81731274 0.81975329 0.81730009]\n",
      "Mean accuracy: 0.8183749470476288\n",
      "Standard deviation: 0.0009885067254048606\n",
      "Accuracy test on train: 0.8209887582804487\n",
      " \n",
      "parameters:  (120, 'log_loss')\n",
      "Accuracy: 0.8179791099383752\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.73      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81899457 0.81867211 0.81828643 0.81927909 0.81853934]\n",
      "Mean accuracy: 0.8187543073197565\n",
      "Standard deviation: 0.00034785036262306994\n",
      "Accuracy test on train: 0.820320361006787\n",
      " \n",
      "parameters:  (140, 'gini')\n",
      "Accuracy: 0.821098287824247\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.77      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82139718 0.82038556 0.82062582 0.82088505 0.82021484]\n",
      "Mean accuracy: 0.8207016900500126\n",
      "Standard deviation: 0.0004147402429704452\n",
      "Accuracy test on train: 0.8226633644228389\n",
      " \n",
      "parameters:  (140, 'entropy')\n",
      "Accuracy: 0.819016025830165\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.74      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81754668 0.81874166 0.81904515 0.81908309 0.82025278]\n",
      "Mean accuracy: 0.818933871181897\n",
      "Standard deviation: 0.0008643403729798386\n",
      "Accuracy test on train: 0.8213663124161116\n",
      " \n",
      "parameters:  (140, 'log_loss')\n",
      "Accuracy: 0.8180676271486499\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.73      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81756565 0.81684486 0.81901354 0.81779326 0.81785649]\n",
      "Mean accuracy: 0.8178147583791201\n",
      "Standard deviation: 0.0006988608389467914\n",
      "Accuracy test on train: 0.8204468145450473\n",
      " \n",
      "parameters:  (160, 'gini')\n",
      "Accuracy: 0.8209886950877163\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.77      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82228236 0.82005045 0.82071434 0.82181448 0.82058788]\n",
      "Mean accuracy: 0.821089902061823\n",
      "Standard deviation: 0.000827102064518132\n",
      "Accuracy test on train: 0.8226127830075348\n",
      " \n",
      "parameters:  (160, 'entropy')\n",
      "Accuracy: 0.8180718422539011\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81824849 0.8185583  0.81876695 0.81954464 0.8185836 ]\n",
      "Mean accuracy: 0.8187403974431117\n",
      "Standard deviation: 0.00043525334684452996\n",
      "Accuracy test on train: 0.8204558469406373\n",
      " \n",
      "parameters:  (160, 'log_loss')\n",
      "Accuracy: 0.8181055630959105\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.74      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.8183054  0.81999987 0.81943083 0.81874166 0.81850772]\n",
      "Mean accuracy: 0.8189970978939183\n",
      "Standard deviation: 0.000628787664286698\n",
      "Accuracy test on train: 0.8210700498407588\n",
      " \n",
      "parameters:  (180, 'gini')\n",
      "Accuracy: 0.8206388413518686\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86     80841\n",
      "           1       0.86      0.86      0.86     78164\n",
      "           2       0.71      0.78      0.74     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.83      0.82      0.82    237242\n",
      "weighted avg       0.83      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.82071434 0.82017059 0.8208155  0.82146041 0.81987342]\n",
      "Mean accuracy: 0.8206068499819803\n",
      "Standard deviation: 0.0005498906786163192\n",
      "Accuracy test on train: 0.8223580694518962\n",
      " \n",
      "parameters:  (180, 'entropy')\n",
      "Accuracy: 0.8182573068849529\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.72      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81817262 0.81949406 0.8185583  0.8197849  0.81840656]\n",
      "Mean accuracy: 0.8188832898122799\n",
      "Standard deviation: 0.0006362203409267634\n",
      "Accuracy test on train: 0.8205118477932954\n",
      " \n",
      "parameters:  (180, 'log_loss')\n",
      "Accuracy: 0.8185481491472842\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     80841\n",
      "           1       0.85      0.87      0.86     78164\n",
      "           2       0.73      0.73      0.73     78237\n",
      "\n",
      "    accuracy                           0.82    237242\n",
      "   macro avg       0.82      0.82      0.82    237242\n",
      "weighted avg       0.82      0.82      0.82    237242\n",
      "\n",
      "Cross-validation scores: [0.81899457 0.81865314 0.81798926 0.81893134 0.81828011]\n",
      "Mean accuracy: 0.8185696853206542\n",
      "Standard deviation: 0.0003844558441357363\n",
      "Accuracy test on train: 0.8210140489881007\n",
      " \n",
      "highest accuracy:  0.8215788098228812\n",
      "cross validation score 0.8196432748907758\n",
      "best parameters accuracy:  (40, 'gini')\n"
     ]
    }
   ],
   "source": [
    "estimators = np.arange(20, 200, 20)\n",
    "Criterions = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "accuracy = []\n",
    "params = []\n",
    "cross_val = []\n",
    "\n",
    "for n_est in estimators:\n",
    "    for criterion in Criterions:\n",
    "\n",
    "        parameters = (n_est, criterion)\n",
    "        print(\"parameters: \", parameters)\n",
    "        params.append(parameters)\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=n_est, criterion=criterion,max_depth= 10, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = rf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(rf, features, label, cv=kf, scoring='accuracy')\n",
    "        cross_val.append(scores.mean())\n",
    "        print(f\"Cross-validation scores: {scores}\")\n",
    "        print(f\"Mean accuracy: {scores.mean()}\")\n",
    "        print(f\"Standard deviation: {scores.std()}\")\n",
    "\n",
    "        y_pred = rf.predict(X_train)\n",
    "        print(\"Accuracy test on train:\", accuracy_score(y_train, y_pred))\n",
    "\n",
    "        print(\" \")\n",
    "            \n",
    "print(\"highest accuracy: \", max(accuracy))\n",
    "\n",
    "max_index = accuracy.index(max(accuracy))\n",
    "best_param = params[max_index]\n",
    "print(\"cross validation score\", cross_val[max_index])\n",
    "print(\"best parameters accuracy: \", best_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e3f8ae",
   "metadata": {},
   "source": [
    "# KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b2936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00fd842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['pu_location', 'do_location', 'passenger_count', 'trip_distance', 'store_and_fwd_flag_N', 'store_and_fwd_flag_Y', 'rate_type_Standard rate', 'rate_type_Negotiated fare', 'hours', 'vendor_Creative Mobile Technologies, LLC', 'vendor_VeriFone Inc.', 'payment_type_Credit card', 'payment_type_Cash', 'trip_type_Street-hail', 'trip_type_Dispatch', 'day_of_week', 'month']]\n",
    "\n",
    "label = df['price_category']\n",
    "label = label.astype('category').cat.codes\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "535f653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 5\n",
      "Accuracy: 0.7839716407718701\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83     80841\n",
      "           1       0.85      0.83      0.84     78164\n",
      "           2       0.68      0.68      0.68     78237\n",
      "\n",
      "    accuracy                           0.78    237242\n",
      "   macro avg       0.78      0.78      0.78    237242\n",
      "weighted avg       0.78      0.78      0.78    237242\n",
      "\n",
      "parameters: 10\n",
      "Accuracy: 0.7934429822712673\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84     80841\n",
      "           1       0.86      0.85      0.85     78164\n",
      "           2       0.70      0.66      0.68     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.79      0.79      0.79    237242\n",
      "weighted avg       0.79      0.79      0.79    237242\n",
      "\n",
      "parameters: 20\n",
      "Accuracy: 0.7961237892110166\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84     80841\n",
      "           1       0.88      0.83      0.85     78164\n",
      "           2       0.70      0.70      0.70     78237\n",
      "\n",
      "    accuracy                           0.80    237242\n",
      "   macro avg       0.80      0.80      0.80    237242\n",
      "weighted avg       0.80      0.80      0.80    237242\n",
      "\n",
      "parameters: 30\n",
      "Accuracy: 0.7945136190050666\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84     80841\n",
      "           1       0.88      0.82      0.85     78164\n",
      "           2       0.69      0.70      0.70     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.80      0.79      0.79    237242\n",
      "weighted avg       0.80      0.79      0.80    237242\n",
      "\n",
      "parameters: 40\n",
      "Accuracy: 0.7920435673278762\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84     80841\n",
      "           1       0.89      0.81      0.85     78164\n",
      "           2       0.68      0.70      0.69     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.80      0.79      0.79    237242\n",
      "weighted avg       0.80      0.79      0.79    237242\n",
      "\n",
      "parameters: 50\n",
      "Accuracy: 0.7898306370710076\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83     80841\n",
      "           1       0.89      0.81      0.85     78164\n",
      "           2       0.68      0.70      0.69     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.79      0.79      0.79    237242\n",
      "weighted avg       0.79      0.79      0.79    237242\n",
      "\n",
      "parameters: 100\n",
      "Accuracy: 0.7822855986713988\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83     80841\n",
      "           1       0.90      0.79      0.84     78164\n",
      "           2       0.67      0.69      0.68     78237\n",
      "\n",
      "    accuracy                           0.78    237242\n",
      "   macro avg       0.79      0.78      0.78    237242\n",
      "weighted avg       0.79      0.78      0.78    237242\n",
      "\n",
      "parameters: 150\n",
      "Accuracy: 0.7778976741049224\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83     80841\n",
      "           1       0.90      0.78      0.83     78164\n",
      "           2       0.66      0.69      0.68     78237\n",
      "\n",
      "    accuracy                           0.78    237242\n",
      "   macro avg       0.78      0.78      0.78    237242\n",
      "weighted avg       0.78      0.78      0.78    237242\n",
      "\n",
      "parameters: 200\n",
      "Accuracy: 0.7729069894875275\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82     80841\n",
      "           1       0.90      0.77      0.83     78164\n",
      "           2       0.66      0.68      0.67     78237\n",
      "\n",
      "    accuracy                           0.77    237242\n",
      "   macro avg       0.78      0.77      0.77    237242\n",
      "weighted avg       0.78      0.77      0.77    237242\n",
      "\n",
      "parameters: 250\n",
      "Accuracy: 0.7692356328137514\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82     80841\n",
      "           1       0.91      0.76      0.83     78164\n",
      "           2       0.65      0.68      0.66     78237\n",
      "\n",
      "    accuracy                           0.77    237242\n",
      "   macro avg       0.78      0.77      0.77    237242\n",
      "weighted avg       0.78      0.77      0.77    237242\n",
      "\n",
      "parameters: 300\n",
      "Accuracy: 0.7653535208774163\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82     80841\n",
      "           1       0.91      0.76      0.82     78164\n",
      "           2       0.65      0.67      0.66     78237\n",
      "\n",
      "    accuracy                           0.77    237242\n",
      "   macro avg       0.77      0.76      0.77    237242\n",
      "weighted avg       0.77      0.77      0.77    237242\n",
      "\n",
      "parameters: 350\n",
      "Accuracy: 0.7626221326746528\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.81     80841\n",
      "           1       0.91      0.75      0.82     78164\n",
      "           2       0.64      0.67      0.65     78237\n",
      "\n",
      "    accuracy                           0.76    237242\n",
      "   macro avg       0.77      0.76      0.76    237242\n",
      "weighted avg       0.77      0.76      0.76    237242\n",
      "\n",
      "parameters: 400\n",
      "Accuracy: 0.7597685064196052\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81     80841\n",
      "           1       0.91      0.75      0.82     78164\n",
      "           2       0.64      0.66      0.65     78237\n",
      "\n",
      "    accuracy                           0.76    237242\n",
      "   macro avg       0.77      0.76      0.76    237242\n",
      "weighted avg       0.77      0.76      0.76    237242\n",
      "\n",
      "parameters: 450\n",
      "Accuracy: 0.7568305780595341\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81     80841\n",
      "           1       0.91      0.74      0.82     78164\n",
      "           2       0.64      0.65      0.65     78237\n",
      "\n",
      "    accuracy                           0.76    237242\n",
      "   macro avg       0.77      0.76      0.76    237242\n",
      "weighted avg       0.77      0.76      0.76    237242\n",
      "\n",
      "parameters: 500\n",
      "Accuracy: 0.7544701191188744\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.81     80841\n",
      "           1       0.91      0.74      0.81     78164\n",
      "           2       0.63      0.65      0.64     78237\n",
      "\n",
      "    accuracy                           0.75    237242\n",
      "   macro avg       0.77      0.75      0.75    237242\n",
      "weighted avg       0.76      0.75      0.76    237242\n",
      "\n",
      "parameters: 550\n",
      "Accuracy: 0.7519199804419117\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.81     80841\n",
      "           1       0.91      0.74      0.81     78164\n",
      "           2       0.63      0.64      0.64     78237\n",
      "\n",
      "    accuracy                           0.75    237242\n",
      "   macro avg       0.76      0.75      0.75    237242\n",
      "weighted avg       0.76      0.75      0.75    237242\n",
      "\n",
      "parameters: 600\n",
      "Accuracy: 0.749466789185726\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.80     80841\n",
      "           1       0.91      0.73      0.81     78164\n",
      "           2       0.63      0.64      0.63     78237\n",
      "\n",
      "    accuracy                           0.75    237242\n",
      "   macro avg       0.76      0.75      0.75    237242\n",
      "weighted avg       0.76      0.75      0.75    237242\n",
      "\n",
      "parameters: 650\n",
      "Accuracy: 0.7474603990861651\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80     80841\n",
      "           1       0.91      0.73      0.81     78164\n",
      "           2       0.63      0.64      0.63     78237\n",
      "\n",
      "    accuracy                           0.75    237242\n",
      "   macro avg       0.76      0.75      0.75    237242\n",
      "weighted avg       0.76      0.75      0.75    237242\n",
      "\n",
      "parameters: 700\n",
      "Accuracy: 0.7460019726692575\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80     80841\n",
      "           1       0.91      0.73      0.81     78164\n",
      "           2       0.62      0.63      0.63     78237\n",
      "\n",
      "    accuracy                           0.75    237242\n",
      "   macro avg       0.76      0.74      0.75    237242\n",
      "weighted avg       0.76      0.75      0.75    237242\n",
      "\n",
      "parameters: 750\n",
      "Accuracy: 0.7444213082000658\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80     80841\n",
      "           1       0.91      0.72      0.81     78164\n",
      "           2       0.62      0.63      0.63     78237\n",
      "\n",
      "    accuracy                           0.74    237242\n",
      "   macro avg       0.76      0.74      0.74    237242\n",
      "weighted avg       0.76      0.74      0.75    237242\n",
      "\n",
      "parameters: 800\n",
      "Accuracy: 0.7428996552043905\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80     80841\n",
      "           1       0.91      0.72      0.80     78164\n",
      "           2       0.62      0.63      0.63     78237\n",
      "\n",
      "    accuracy                           0.74    237242\n",
      "   macro avg       0.76      0.74      0.74    237242\n",
      "weighted avg       0.76      0.74      0.74    237242\n",
      "\n",
      "parameters: 850\n",
      "Accuracy: 0.7413948626297199\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80     80841\n",
      "           1       0.91      0.72      0.80     78164\n",
      "           2       0.62      0.63      0.62     78237\n",
      "\n",
      "    accuracy                           0.74    237242\n",
      "   macro avg       0.75      0.74      0.74    237242\n",
      "weighted avg       0.75      0.74      0.74    237242\n",
      "\n",
      "parameters: 900\n",
      "Accuracy: 0.7401387612648688\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.87      0.80     80841\n",
      "           1       0.91      0.71      0.80     78164\n",
      "           2       0.62      0.63      0.62     78237\n",
      "\n",
      "    accuracy                           0.74    237242\n",
      "   macro avg       0.75      0.74      0.74    237242\n",
      "weighted avg       0.75      0.74      0.74    237242\n",
      "\n",
      "parameters: 1000\n",
      "Accuracy: 0.7376644944824272\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.87      0.80     80841\n",
      "           1       0.91      0.71      0.80     78164\n",
      "           2       0.61      0.63      0.62     78237\n",
      "\n",
      "    accuracy                           0.74    237242\n",
      "   macro avg       0.75      0.74      0.74    237242\n",
      "weighted avg       0.75      0.74      0.74    237242\n",
      "\n",
      "parameters: 1500\n",
      "Accuracy: 0.7285893728766407\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.87      0.79     80841\n",
      "           1       0.92      0.69      0.79     78164\n",
      "           2       0.60      0.62      0.61     78237\n",
      "\n",
      "    accuracy                           0.73    237242\n",
      "   macro avg       0.75      0.73      0.73    237242\n",
      "weighted avg       0.75      0.73      0.73    237242\n",
      "\n",
      "highest accuracy:  0.7961237892110166\n",
      "best parameters accuracy:  20\n",
      "lowest accuracy:  0.7285893728766407\n",
      "worst parameters:  1500\n"
     ]
    }
   ],
   "source": [
    "nNeighbours = [5, 10, 20, 30, 40, 50, 100, 150, 200, 250, 300, 350, 400, 450,500, 550, 600, 650, 700, 750, 800, 850, 900, 1000, 1500]\n",
    "\n",
    "params = []\n",
    "accuracy = []\n",
    "for n in nNeighbours:\n",
    "    parameters = (n)\n",
    "    params.append(parameters)\n",
    "    print(f\"parameters: {parameters}\")\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=n, n_jobs=-1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"highest accuracy: \", max(accuracy))\n",
    "\n",
    "max_index = accuracy.index(max(accuracy))\n",
    "best_param = params[max_index]\n",
    "print(\"best parameters accuracy: \", best_param)\n",
    "\n",
    "print(\"lowest accuracy: \", min(accuracy))\n",
    "\n",
    "min_index = accuracy.index(min(accuracy))\n",
    "worst_param = params[min_index]\n",
    "print(\"worst parameters: \", worst_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9608917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test on train for best parameters: 0.8178238068657046\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.22 MiB for an array with shape (553563,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39mworst_param, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> 10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy test on train for worst parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_train, y_pred))\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:249\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ArgKminClassMode\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    247\u001b[0m         X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\n\u001b[0;32m    248\u001b[0m     ):\n\u001b[1;32m--> 249\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_2d_:\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m    252\u001b[0m                 [\n\u001b[0;32m    253\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[idx][np\u001b[38;5;241m.\u001b[39margmax(probas, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m                 axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    257\u001b[0m             )\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:375\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# a simple ':' index doesn't work right\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pred_labels\u001b[38;5;241m.\u001b[39mT):  \u001b[38;5;66;03m# loop is O(n_neighbors)\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m     proba_k[all_rows, idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weights[:, i]\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# normalize 'votes' into real [0,1] probabilities\u001b[39;00m\n\u001b[0;32m    378\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m proba_k\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.22 MiB for an array with shape (553563,) and data type float64"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=best_param, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_train)\n",
    "print(\"Accuracy test on train for best parameters:\", accuracy_score(y_train, y_pred))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=worst_param, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_train)\n",
    "print(\"Accuracy test on train for worst parameters:\", accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5de2cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 5\n",
      "Accuracy: 0.7845659706122862\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83     80841\n",
      "           1       0.85      0.83      0.84     78164\n",
      "           2       0.68      0.69      0.68     78237\n",
      "\n",
      "    accuracy                           0.78    237242\n",
      "   macro avg       0.79      0.78      0.78    237242\n",
      "weighted avg       0.79      0.78      0.79    237242\n",
      "\n",
      "parameters: 10\n",
      "Accuracy: 0.7946948685308672\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84     80841\n",
      "           1       0.87      0.83      0.85     78164\n",
      "           2       0.69      0.71      0.70     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.80      0.79      0.80    237242\n",
      "weighted avg       0.80      0.79      0.80    237242\n",
      "\n",
      "parameters: 20\n",
      "Accuracy: 0.7998541573583092\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84     80841\n",
      "           1       0.88      0.83      0.85     78164\n",
      "           2       0.69      0.72      0.71     78237\n",
      "\n",
      "    accuracy                           0.80    237242\n",
      "   macro avg       0.80      0.80      0.80    237242\n",
      "weighted avg       0.80      0.80      0.80    237242\n",
      "\n",
      "parameters: 30\n",
      "Accuracy: 0.8007688351978149\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84     80841\n",
      "           1       0.88      0.82      0.85     78164\n",
      "           2       0.69      0.73      0.71     78237\n",
      "\n",
      "    accuracy                           0.80    237242\n",
      "   macro avg       0.80      0.80      0.80    237242\n",
      "weighted avg       0.80      0.80      0.80    237242\n",
      "\n",
      "parameters: 40\n",
      "Accuracy: 0.8001365694101382\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84     80841\n",
      "           1       0.89      0.82      0.85     78164\n",
      "           2       0.69      0.73      0.71     78237\n",
      "\n",
      "    accuracy                           0.80    237242\n",
      "   macro avg       0.80      0.80      0.80    237242\n",
      "weighted avg       0.80      0.80      0.80    237242\n",
      "\n",
      "parameters: 50\n",
      "Accuracy: 0.7994579374646985\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84     80841\n",
      "           1       0.89      0.82      0.85     78164\n",
      "           2       0.69      0.72      0.71     78237\n",
      "\n",
      "    accuracy                           0.80    237242\n",
      "   macro avg       0.80      0.80      0.80    237242\n",
      "weighted avg       0.80      0.80      0.80    237242\n",
      "\n",
      "parameters: 100\n",
      "Accuracy: 0.795234402003018\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84     80841\n",
      "           1       0.90      0.80      0.85     78164\n",
      "           2       0.68      0.72      0.70     78237\n",
      "\n",
      "    accuracy                           0.80    237242\n",
      "   macro avg       0.80      0.79      0.80    237242\n",
      "weighted avg       0.80      0.80      0.80    237242\n",
      "\n",
      "parameters: 150\n",
      "Accuracy: 0.7914787432242183\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84     80841\n",
      "           1       0.90      0.79      0.84     78164\n",
      "           2       0.68      0.72      0.70     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.80      0.79      0.79    237242\n",
      "weighted avg       0.80      0.79      0.79    237242\n",
      "\n",
      "parameters: 200\n",
      "Accuracy: 0.7879001188659681\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84     80841\n",
      "           1       0.90      0.78      0.84     78164\n",
      "           2       0.67      0.71      0.69     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.79      0.79      0.79    237242\n",
      "weighted avg       0.79      0.79      0.79    237242\n",
      "\n",
      "parameters: 250\n",
      "Accuracy: 0.7853331197680006\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83     80841\n",
      "           1       0.90      0.78      0.84     78164\n",
      "           2       0.67      0.71      0.69     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.79      0.78      0.79    237242\n",
      "weighted avg       0.79      0.79      0.79    237242\n",
      "\n",
      "parameters: 300\n",
      "Accuracy: 0.7828167019330473\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83     80841\n",
      "           1       0.90      0.77      0.83     78164\n",
      "           2       0.67      0.70      0.69     78237\n",
      "\n",
      "    accuracy                           0.78    237242\n",
      "   macro avg       0.79      0.78      0.78    237242\n",
      "weighted avg       0.79      0.78      0.78    237242\n",
      "\n",
      "parameters: 350\n",
      "Accuracy: 0.7807302248337141\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83     80841\n",
      "           1       0.91      0.77      0.83     78164\n",
      "           2       0.66      0.70      0.68     78237\n",
      "\n",
      "    accuracy                           0.78    237242\n",
      "   macro avg       0.79      0.78      0.78    237242\n",
      "weighted avg       0.79      0.78      0.78    237242\n",
      "\n",
      "parameters: 400\n",
      "Accuracy: 0.7784034867350638\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83     80841\n",
      "           1       0.91      0.76      0.83     78164\n",
      "           2       0.66      0.70      0.68     78237\n",
      "\n",
      "    accuracy                           0.78    237242\n",
      "   macro avg       0.79      0.78      0.78    237242\n",
      "weighted avg       0.79      0.78      0.78    237242\n",
      "\n",
      "parameters: 450\n",
      "Accuracy: 0.7762579981622141\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83     80841\n",
      "           1       0.91      0.76      0.83     78164\n",
      "           2       0.66      0.69      0.68     78237\n",
      "\n",
      "    accuracy                           0.78    237242\n",
      "   macro avg       0.78      0.78      0.78    237242\n",
      "weighted avg       0.78      0.78      0.78    237242\n",
      "\n",
      "parameters: 500\n",
      "Accuracy: 0.7743106195361699\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83     80841\n",
      "           1       0.91      0.76      0.82     78164\n",
      "           2       0.66      0.69      0.67     78237\n",
      "\n",
      "    accuracy                           0.77    237242\n",
      "   macro avg       0.78      0.77      0.77    237242\n",
      "weighted avg       0.78      0.77      0.78    237242\n",
      "\n",
      "parameters: 550\n",
      "Accuracy: 0.7725950716989404\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.83     80841\n",
      "           1       0.91      0.75      0.82     78164\n",
      "           2       0.66      0.69      0.67     78237\n",
      "\n",
      "    accuracy                           0.77    237242\n",
      "   macro avg       0.78      0.77      0.77    237242\n",
      "weighted avg       0.78      0.77      0.77    237242\n",
      "\n",
      "parameters: 600\n",
      "Accuracy: 0.7710312676507532\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82     80841\n",
      "           1       0.91      0.75      0.82     78164\n",
      "           2       0.65      0.68      0.67     78237\n",
      "\n",
      "    accuracy                           0.77    237242\n",
      "   macro avg       0.78      0.77      0.77    237242\n",
      "weighted avg       0.78      0.77      0.77    237242\n",
      "\n",
      "parameters: 650\n",
      "Accuracy: 0.7691639760244813\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82     80841\n",
      "           1       0.91      0.75      0.82     78164\n",
      "           2       0.65      0.68      0.67     78237\n",
      "\n",
      "    accuracy                           0.77    237242\n",
      "   macro avg       0.78      0.77      0.77    237242\n",
      "weighted avg       0.78      0.77      0.77    237242\n",
      "\n",
      "parameters: 700\n",
      "Accuracy: 0.7677392704495831\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82     80841\n",
      "           1       0.91      0.74      0.82     78164\n",
      "           2       0.65      0.68      0.66     78237\n",
      "\n",
      "    accuracy                           0.77    237242\n",
      "   macro avg       0.78      0.77      0.77    237242\n",
      "weighted avg       0.78      0.77      0.77    237242\n",
      "\n",
      "parameters: 750\n",
      "Accuracy: 0.7661459606646378\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82     80841\n",
      "           1       0.91      0.74      0.82     78164\n",
      "           2       0.65      0.68      0.66     78237\n",
      "\n",
      "    accuracy                           0.77    237242\n",
      "   macro avg       0.78      0.76      0.77    237242\n",
      "weighted avg       0.78      0.77      0.77    237242\n",
      "\n",
      "parameters: 800\n",
      "Accuracy: 0.7646074472479578\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82     80841\n",
      "           1       0.91      0.74      0.81     78164\n",
      "           2       0.65      0.68      0.66     78237\n",
      "\n",
      "    accuracy                           0.76    237242\n",
      "   macro avg       0.78      0.76      0.77    237242\n",
      "weighted avg       0.77      0.76      0.77    237242\n",
      "\n",
      "parameters: 850\n",
      "Accuracy: 0.7636927694084521\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82     80841\n",
      "           1       0.91      0.74      0.81     78164\n",
      "           2       0.64      0.68      0.66     78237\n",
      "\n",
      "    accuracy                           0.76    237242\n",
      "   macro avg       0.77      0.76      0.76    237242\n",
      "weighted avg       0.77      0.76      0.76    237242\n",
      "\n",
      "parameters: 900\n",
      "Accuracy: 0.762411377412094\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82     80841\n",
      "           1       0.91      0.73      0.81     78164\n",
      "           2       0.64      0.67      0.66     78237\n",
      "\n",
      "    accuracy                           0.76    237242\n",
      "   macro avg       0.77      0.76      0.76    237242\n",
      "weighted avg       0.77      0.76      0.76    237242\n",
      "\n",
      "parameters: 1000\n",
      "Accuracy: 0.7606452483118503\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82     80841\n",
      "           1       0.91      0.73      0.81     78164\n",
      "           2       0.64      0.67      0.66     78237\n",
      "\n",
      "    accuracy                           0.76    237242\n",
      "   macro avg       0.77      0.76      0.76    237242\n",
      "weighted avg       0.77      0.76      0.76    237242\n",
      "\n",
      "parameters: 1500\n",
      "Accuracy: 0.7518483236526416\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.88      0.81     80841\n",
      "           1       0.92      0.71      0.80     78164\n",
      "           2       0.63      0.66      0.64     78237\n",
      "\n",
      "    accuracy                           0.75    237242\n",
      "   macro avg       0.77      0.75      0.75    237242\n",
      "weighted avg       0.77      0.75      0.75    237242\n",
      "\n",
      "highest accuracy:  0.8007688351978149\n",
      "best parameters accuracy:  30\n",
      "lowest accuracy:  0.7518483236526416\n",
      "worst parameters:  1500\n"
     ]
    }
   ],
   "source": [
    "nNeighbours = [5, 10, 20, 30, 40, 50, 100, 150, 200, 250, 300, 350, 400, 450,500, 550, 600, 650, 700, 750, 800, 850, 900, 1000, 1500]\n",
    "\n",
    "params = []\n",
    "accuracy = []\n",
    "for n in nNeighbours:\n",
    "    parameters = (n)\n",
    "    params.append(parameters)\n",
    "    print(f\"parameters: {parameters}\")\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=n, weights = 'distance', n_jobs=-1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     scores = cross_val_score(knn, features_scaled, label, cv=kf, scoring='accuracy')\n",
    "#     accuracy.append(scores.mean())\n",
    "#     print(f\"Cross-validation scores: {scores}\")\n",
    "#     print(f\"Mean accuracy: {scores.mean()}\")\n",
    "#     print(f\"Standard deviation: {scores.std()}\")\n",
    "\n",
    "print(\"highest accuracy: \", max(accuracy))\n",
    "\n",
    "max_index = accuracy.index(max(accuracy))\n",
    "best_param = params[max_index]\n",
    "print(\"best parameters accuracy: \", best_param)\n",
    "\n",
    "print(\"lowest accuracy: \", min(accuracy))\n",
    "\n",
    "min_index = accuracy.index(min(accuracy))\n",
    "worst_param = params[min_index]\n",
    "print(\"worst parameters: \", worst_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "714f168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: euclidean\n",
      "Accuracy: 0.8007688351978149\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84     80841\n",
      "           1       0.88      0.82      0.85     78164\n",
      "           2       0.69      0.73      0.71     78237\n",
      "\n",
      "    accuracy                           0.80    237242\n",
      "   macro avg       0.80      0.80      0.80    237242\n",
      "weighted avg       0.80      0.80      0.80    237242\n",
      "\n",
      "parameters: manhattan\n",
      "Accuracy: 0.806025071446034\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85     80841\n",
      "           1       0.88      0.83      0.85     78164\n",
      "           2       0.70      0.73      0.72     78237\n",
      "\n",
      "    accuracy                           0.81    237242\n",
      "   macro avg       0.81      0.81      0.81    237242\n",
      "weighted avg       0.81      0.81      0.81    237242\n",
      "\n",
      "parameters: chebyshev\n",
      "Accuracy: 0.7880265720235035\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83     80841\n",
      "           1       0.88      0.81      0.85     78164\n",
      "           2       0.68      0.71      0.69     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.79      0.79      0.79    237242\n",
      "weighted avg       0.79      0.79      0.79    237242\n",
      "\n",
      "parameters: cosine\n",
      "Accuracy: 0.7947876008463931\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.84     80841\n",
      "           1       0.87      0.83      0.85     78164\n",
      "           2       0.69      0.71      0.70     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.80      0.79      0.80    237242\n",
      "weighted avg       0.80      0.79      0.80    237242\n",
      "\n",
      "parameters: hamming\n",
      "Accuracy: 0.6592256008632535\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.72     80841\n",
      "           1       0.81      0.56      0.66     78164\n",
      "           2       0.58      0.58      0.58     78237\n",
      "\n",
      "    accuracy                           0.66    237242\n",
      "   macro avg       0.68      0.66      0.66    237242\n",
      "weighted avg       0.68      0.66      0.66    237242\n",
      "\n",
      "highest accuracy:  0.806025071446034\n",
      "best parameters accuracy:  manhattan\n",
      "lowest accuracy:  0.6592256008632535\n",
      "worst parameters:  hamming\n"
     ]
    }
   ],
   "source": [
    "# nNeighbours = [5, 10, 20, 30, 40, 50, 100, 150, 200, 250, 300, 350, 400, 450,500, 550, 600, 650, 700, 750, 800, 850, 900, 1000, 1500]\n",
    "metric = ['euclidean', 'manhattan', 'chebyshev', 'cosine', 'hamming']\n",
    "params = []\n",
    "accuracy = []\n",
    "for m in metric:\n",
    "    parameters = (m)\n",
    "    params.append(parameters)\n",
    "    print(f\"parameters: {parameters}\")\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=30, weights = 'distance', metric = m, n_jobs=-1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     scores = cross_val_score(knn, features_scaled, label, cv=kf, scoring='accuracy')\n",
    "#     accuracy.append(scores.mean())\n",
    "#     print(f\"Cross-validation scores: {scores}\")\n",
    "#     print(f\"Mean accuracy: {scores.mean()}\")\n",
    "#     print(f\"Standard deviation: {scores.std()}\")\n",
    "\n",
    "print(\"highest accuracy: \", max(accuracy))\n",
    "\n",
    "max_index = accuracy.index(max(accuracy))\n",
    "best_param = params[max_index]\n",
    "print(\"best parameters accuracy: \", best_param)\n",
    "\n",
    "print(\"lowest accuracy: \", min(accuracy))\n",
    "\n",
    "min_index = accuracy.index(min(accuracy))\n",
    "worst_param = params[min_index]\n",
    "print(\"worst parameters: \", worst_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7b8cc",
   "metadata": {},
   "source": [
    "### unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7bc2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['pu_location', 'do_location', 'passenger_count', 'trip_distance', 'store_and_fwd_flag_N', 'store_and_fwd_flag_Y', 'rate_type_Standard rate', 'rate_type_Negotiated fare', 'hours', 'vendor_Creative Mobile Technologies, LLC', 'vendor_VeriFone Inc.', 'payment_type_Credit card', 'payment_type_Cash', 'trip_type_Street-hail', 'trip_type_Dispatch', 'day_of_week', 'month']]\n",
    "\n",
    "label = df['price_category']\n",
    "label = label.astype('category').cat.codes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be8fd6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7714527781758711\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81     80841\n",
      "           1       0.87      0.80      0.83     78164\n",
      "           2       0.66      0.68      0.67     78237\n",
      "\n",
      "    accuracy                           0.77    237242\n",
      "   macro avg       0.77      0.77      0.77    237242\n",
      "weighted avg       0.77      0.77      0.77    237242\n",
      "\n",
      "Cross-validation scores: [0.7751342  0.77353456 0.77358514 0.7729023  0.77377482]\n",
      "Mean accuracy: 0.7737862051959712\n",
      "Standard deviation: 0.0007351545748557183\n",
      "Accuracy test on train: 0.9968422745017279\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=30, weights = 'distance', leaf_size= 20, metric = 'manhattan', n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(knn, features, label, cv=kf, scoring='accuracy')\n",
    "accuracy.append(scores.mean())\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation: {scores.std()}\")\n",
    "\n",
    "y_pred = knn.predict(X_train)\n",
    "\n",
    "print(\"Accuracy test on train:\", accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5edfc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad6557d1",
   "metadata": {},
   "source": [
    "# Naive Bayes Calssification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f3a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = df[['pu_location', 'do_location', 'passenger_count', 'trip_distance', 'store_and_fwd_flag_N', 'store_and_fwd_flag_Y', 'rate_type_Standard rate', 'rate_type_Negotiated fare', 'hours', 'vendor_Creative Mobile Technologies, LLC', 'vendor_VeriFone Inc.', 'payment_type_Credit card', 'payment_type_Cash', 'trip_type_Street-hail', 'trip_type_Dispatch', 'day_of_week', 'month']]\n",
    "label = df['price_category']\n",
    "label = label.astype('category').cat.codes\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# # Apply PCA\n",
    "# pca = PCA()\n",
    "# principal_components = pca.fit_transform(features)\n",
    "# features_pca = pd.DataFrame(data=principal_components)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.3)#, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d4c8f",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8f20f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49390495780679644\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.00      0.00     80664\n",
      "           1       0.91      0.52      0.66     78298\n",
      "           2       0.40      0.98      0.56     78280\n",
      "\n",
      "    accuracy                           0.49    237242\n",
      "   macro avg       0.56      0.50      0.41    237242\n",
      "weighted avg       0.56      0.49      0.41    237242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf12dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.49592504 0.49232744 0.49569742 0.49503354 0.49405353]\n",
      "Mean accuracy: 0.49460739373170376\n",
      "Standard deviation: 0.001312319719645973\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(gnb, features, label, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation: {scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8e497",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d58fa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6754579711855405\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.75     80664\n",
      "           1       0.81      0.68      0.74     78298\n",
      "           2       0.53      0.56      0.54     78280\n",
      "\n",
      "    accuracy                           0.68    237242\n",
      "   macro avg       0.68      0.67      0.68    237242\n",
      "weighted avg       0.68      0.68      0.68    237242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "611b2f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.6761591  0.67512851 0.67600736 0.67347197 0.67252989]\n",
      "Mean accuracy: 0.6746593660889852\n",
      "Standard deviation: 0.0014304526244244514\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(mnb, features, label, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation: {scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a5b92",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f5b75b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3587644683487747\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.56      0.44     80664\n",
      "           1       0.40      0.12      0.19     78298\n",
      "           2       0.35      0.39      0.37     78280\n",
      "\n",
      "    accuracy                           0.36    237242\n",
      "   macro avg       0.37      0.36      0.33    237242\n",
      "weighted avg       0.37      0.36      0.33    237242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a64d9c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.35666188 0.35869778 0.35669982 0.35941857 0.35876733]\n",
      "Mean accuracy: 0.3580490765738709\n",
      "Standard deviation: 0.001145165447656292\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(bnb, features, label, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation: {scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81962d9",
   "metadata": {},
   "source": [
    "## Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4835c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6190387873985214\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.94      0.70     80664\n",
      "           1       0.71      0.84      0.77     78298\n",
      "           2       0.57      0.07      0.12     78280\n",
      "\n",
      "    accuracy                           0.62    237242\n",
      "   macro avg       0.61      0.62      0.53    237242\n",
      "weighted avg       0.61      0.62      0.53    237242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "cpnb = ComplementNB()\n",
    "cpnb.fit(X_train, y_train)\n",
    "y_pred = cpnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ccec488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.62028566 0.61951429 0.61796524 0.62087999 0.61785143]\n",
      "Mean accuracy: 0.6192993215773801\n",
      "Standard deviation: 0.0012160388772750343\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(cpnb, features, label, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation: {scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcf30a",
   "metadata": {},
   "source": [
    "## Categorical Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18430aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7608054223113951\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.81     80664\n",
      "           1       0.82      0.78      0.80     78298\n",
      "           2       0.62      0.75      0.68     78280\n",
      "\n",
      "    accuracy                           0.76    237242\n",
      "   macro avg       0.78      0.76      0.76    237242\n",
      "weighted avg       0.78      0.76      0.77    237242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "catnb = CategoricalNB()\n",
    "catnb.fit(X_train, y_train)\n",
    "y_pred = catnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22f72dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.75977643 0.75935281 0.76168588 0.76007992 0.75963101]\n",
      "Mean accuracy: 0.7601052092488034\n",
      "Standard deviation: 0.0008244045612557889\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(catnb, features, label, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation: {scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fe07f",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b71a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'None'],\n",
    "    'solver': ['lbfgs', 'saga', 'sag', 'newton-cg']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200776fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "220 fits failed out of a total of 320.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1178, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1', 'none' (deprecated)} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'none' (deprecated), 'l2'} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'none' (deprecated), 'l2', 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.79442087        nan        nan 0.77337901 0.79441906\n",
      " 0.79455636 0.79472255        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.79444977\n",
      "        nan        nan 0.77002615 0.79444977 0.79457442 0.79476591\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.79445881        nan        nan\n",
      " 0.76783491 0.79444616 0.79457442 0.79478758        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.79444977        nan        nan 0.76811131 0.79444797\n",
      " 0.79455997 0.79478578        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Accuracy: 0.7942059163217305\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85     80841\n",
      "           1       0.87      0.80      0.84     78164\n",
      "           2       0.69      0.70      0.69     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.80      0.79      0.79    237242\n",
      "weighted avg       0.80      0.79      0.79    237242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=100)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, verbose=3, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a59af68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7948237147352695\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(C = 10, penalty= 'l2', solver= 'newton-cg', max_iter=100)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b26d464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7948237147352695\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85    188017\n",
      "           1       0.87      0.80      0.84    182815\n",
      "           2       0.69      0.70      0.70    182731\n",
      "\n",
      "    accuracy                           0.79    553563\n",
      "   macro avg       0.80      0.79      0.79    553563\n",
      "weighted avg       0.80      0.79      0.80    553563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search.predict(X_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "578d0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "features = df[['pu_location', 'do_location', 'passenger_count', 'trip_distance', 'store_and_fwd_flag_N', 'store_and_fwd_flag_Y', 'rate_type_Standard rate', 'rate_type_Negotiated fare', 'hours', 'vendor_Creative Mobile Technologies, LLC', 'vendor_VeriFone Inc.', 'payment_type_Credit card', 'payment_type_Cash', 'trip_type_Street-hail', 'trip_type_Dispatch', 'day_of_week', 'month']]\n",
    "\n",
    "label = df['price_category']\n",
    "label = label.astype('category').cat.codes\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dde69e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "220 fits failed out of a total of 320.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1178, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1', 'none' (deprecated)} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "16 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'none' (deprecated), 'elasticnet'} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "7 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'none' (deprecated), 'elasticnet', 'l2'} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'none' (deprecated), 'l2', 'elasticnet', 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'none' (deprecated), 'l1', 'elasticnet'} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "7 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'none' (deprecated), 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.794737          nan        nan 0.79468281 0.79465391\n",
      " 0.79468281 0.794681          nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.79477675\n",
      "        nan        nan 0.79476591 0.79477133 0.79476771 0.79477494\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.79478758        nan        nan\n",
      " 0.79478758 0.79478578 0.79479842 0.79478397        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.79478578        nan        nan 0.7947912  0.79478758\n",
      " 0.794793   0.79478578        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 10, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Accuracy: 0.7942143465322329\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85     80841\n",
      "           1       0.87      0.80      0.84     78164\n",
      "           2       0.69      0.70      0.69     78237\n",
      "\n",
      "    accuracy                           0.79    237242\n",
      "   macro avg       0.80      0.79      0.79    237242\n",
      "weighted avg       0.80      0.79      0.79    237242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=100)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, verbose=3, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "438e4ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7948201017770335\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85    188017\n",
      "           1       0.87      0.80      0.84    182815\n",
      "           2       0.69      0.70      0.70    182731\n",
      "\n",
      "    accuracy                           0.79    553563\n",
      "   macro avg       0.80      0.79      0.79    553563\n",
      "weighted avg       0.80      0.79      0.80    553563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search.predict(X_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b9f23",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff716e23",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe9005",
   "metadata": {},
   "source": [
    "I tested this model using (gini, entropy, log_loss) criterions. After testing the model on the test dataset and applying cross validation for each criterion, I tried to predict the output using the train dataset and to compare the accuracy with that when predicting using the testing data to check if the model was over fitting or not. the accuracy of the prediction using the train set was much higher than that of the prediction using test set, which was indicating that there was overfitting. So I used regularization in the parameter set and forced the values of the max_depth and the min_samples_split so as to prevent overfitting while getting the best accuracy possible. Using Cross Validation, it was found that the best parameter was criterion = 'gini'. the train set accuracy with this parameter was 82.20%, the Cross Validation accuracy was 82.23%, while the train set accuracy was 82.57%. I tried afterthat to scale the data and fit the model again, but it seems like it has no effect on the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953d9fe",
   "metadata": {},
   "source": [
    "## Random Forest Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ae0e2",
   "metadata": {},
   "source": [
    "The Random Forest Classifier provided similar results to those of the Decision Tree. \n",
    "the highest accuracy was 82.11% using the test set, 82.05% using Cross Validation, and 82.26% using the train set. This accuracy was achived using the parameters (n_estimators= 60, criterion= 'gini'). max_depth was also made 10 to prevent over fitting of the model. Scaling here once more did not have any recognized effect on the accuracy of the model, as with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0700f9",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2aac8",
   "metadata": {},
   "source": [
    "The highest accuracy that was achived by the parameters (n_neighhbours = 30, weights = 'distance', metric = 'manhattan'), and this recorded an accuracy of 77.14% using the test set, and 77.37% using CV. However, the accuracy on the train set was 99.68%, which means that the KNN model is significantly overfitting.\n",
    "When applying scaling on the data using the same parameters, the accuracy increased to 79.61% on the test set, and 81.78% on the train set. which means also that the overfitting decreased after scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aebef7",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70285a",
   "metadata": {},
   "source": [
    "For Naive Bayes, we tested two models. Namely, Gaussian Naive Bayes and Categorical Naive Bayes.\n",
    "For Gaussian Naive Bayes, we observed a train set accuracy of 49.68% and a test set accuracy of 49.58%. This corresponds to a significantly low bias, where the model is severely underfit, barely performing better than a completely random guessing model (Which should achieve accuracy ~33%). We think that the reason behind the failure of this model is due to the fact that most features in our dataset are not Gaussian distributed, and that some features may be correlated / dependant, which goes against the assumption by the Naive Bayes' model.\n",
    "For Categorical Naive Bayes, we were able to achieve significantly better (albeit still relatively bad!) results, with a train set accuracy of 76.39% and a test set accuracy of 76.43%.\n",
    "Overall, we do NOT recommend Naive Bayes at all as a model for this set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466e67b",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1d382",
   "metadata": {},
   "source": [
    "Using the Logistic Regression model, the best parameters found to provide the highest accuracy was (C = 10, penalty = 'l2', solver = 'newton-cg'). These parameters provided a CV accuracy of 79.42%, the train set accuracy was 79.48%, meaning that there is no overfitting as the model uses l2 regularization scheme. I then tried to apply scaling on the data using the same parameters, but no change in the accuracy was recorded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b17ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
